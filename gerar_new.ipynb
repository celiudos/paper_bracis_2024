{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criar um Dataset Anotado Sintético com dados para avaliar LGPD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fontes\n",
    "\n",
    "**GOLD**\n",
    "\n",
    "- 2024 Artigo - Combining prompt‑based language models and weak supervision for labeling named entity recognition on legal documents\n",
    "- 2024 Artigo - DODFMiner: An automated tool for Named Entity Recognition from Official Gazettes\n",
    "- 2024 Artigo - Artigo - Legal Document Segmentation and Labeling Through Named Entity Recognition\n",
    "\n",
    "**BRONZE**\n",
    "\n",
    "- 2022 Artigo - Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI (...We come to the conclusion that, although classifiers trained on such synthetic data perform much better than random baselines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Medium - Pierre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonte: [Post Medium Pierre](https://medium.com/@pierre_guillou/nlp-modelos-e-web-app-para-reconhecimento-de-entidade-nomeada-ner-no-dom%C3%ADnio-jur%C3%ADdico-b658db55edfb)\n",
    "\n",
    "- [Modelo HF](https://huggingface.co/pierreguillou/ner-bert-large-cased-pt-lenerbr)\n",
    "\n",
    "Modelos de linguagem natural especializados no domínio jurídico brasileiro Para obter o modelo de linguagem natural especializado no domínio jurídico brasileiro, usamos o notebook e dataset a seguir:\n",
    "\n",
    "- notebook: [Finetuning_language_model_BERtimbau_LeNER_Br.ipynb](https://github.com/piegu/language-models/blob/master/Finetuning_language_model_BERtimbau_LeNER_Br.ipynb)\n",
    "- dataset: [pierreguillou/lener_br_finetuning_language_model](https://huggingface.co/datasets/pierreguillou/lener_br_finetuning_language_model)\n",
    "\n",
    "Modelos NER especializados no domínio jurídico brasileiro Para obter o modelo NER especializado no domínio jurídico brasileiro, usamos o notebook e dataset a seguir:\n",
    "\n",
    "- notebook: [HuggingFace_Notebook_token_classification_NER_LeNER_Br.ipynb](https://github.com/piegu/language-models/blob/master/HuggingFace_Notebook_token_classification_NER_LeNER_Br.ipynb)\n",
    "- dataset: [lener_br](https://huggingface.co/datasets/lener_br)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HF IOB](https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Alterar função fake para incluir dados com digitação errada\n",
    "- Criar Docker para publicar no .mpf\n",
    "- Criar Gradio com ajuste de texto e exportar PDF tarjado\n",
    "- Criar apresentação e cronograma p/ Sandra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar o Dataset LenerBR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celiudos/miniconda3/envs/trc/lib/python3.9/site-packages/datasets/load.py:1454: FutureWarning: The repository for lener_br contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/lener_br\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lener_br\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '422', 'tokens': ['106', ',', 'II', ',', \"''\", 'b', \"''\", ',', '151', 'e', '172', 'do', 'CTN', '-', 'os', 'quais', 'se', 'referem', 'à', 'tese', 'da', 'impossibilidade', 'de', 'remissão', 'e', 'de', 'suspensão', 'da', 'exigibilidade', 'dos', 'créditos', 'tributários', 'pelo', 'aludido', 'convênio', 'do', 'CONFAZ', '-', 'não', 'só', 'podem', 'como', 'devem', 'obrigatoriamente', 'ser', 'objeto', 'de', 'outra', 'demanda', ',', 'PODER', 'JUDICIÁRIO', 'DO', 'ESTADO', 'DO', 'ACRE', 'Segunda', 'Câmara', 'Cível', '13', 'Endereço', ':', 'Rua', 'Tribunal', 'de', 'Justiça', ',', 's/n', ',', 'Via', 'Verde', ',', 'CEP', '69.915-631', ',', 'Tel.', '68', '3302-0444/0445', ',', 'Rio', 'BrancoAC', '-', 'Mod', '.', '500244', '-', 'Autos', 'n.º', '0715337-93.2014.8.01.0001', 'porquanto', 'extrapolam', 'os', 'limites', 'objetivos', 'da', 'causa', 'de', 'pedir', 'inicial', '.'], 'ner_tags': [9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 1, 2, 2, 0, 0, 0, 7, 8, 8, 8, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 11, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# \" \".join(dataset[\"train\"][\"tokens\"][422])\n",
    "# dataset[\"train\"][\"tokens\"][422]\n",
    "print(dataset[\"train\"][422])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106          ,            II           ,            ''           b            ''           ,            151          e            172          do           CTN          - os quais se referem à tese da impossibilidade de remissão e de suspensão da exigibilidade dos créditos tributários pelo aludido convênio do CONFAZ        - não só podem como devem obrigatoriamente ser objeto de outra demanda , PODER JUDICIÁRIO DO ESTADO  DO      ACRE    Segunda       Câmara        Cível         13 Endereço : Rua     Tribunal de      Justiça , s/n , Via     Verde   , CEP 69.915-631 , Tel. 68 3302-0444/0445 , Rio     BrancoAC - Mod . 500244 - Autos            n.º              0715337-93.2014.8.01.0001 porquanto extrapolam os limites objetivos da causa de pedir inicial . \n",
      "B-LEGISLACAO I-LEGISLACAO I-LEGISLACAO I-LEGISLACAO I-LEGISLACAO I-LEGISLACAO I-LEGISLACAO I-LEGISLACAO I-LEGISLACAO I-LEGISLACAO I-LEGISLACAO I-LEGISLACAO I-LEGISLACAO O O  O     O  O       O O    O  O               O  O        O O  O         O  O             O   O        O           O    O       O        O  B-ORGANIZACAO O O   O  O     O    O     O                O   O      O  O     O       O O     O          O  B-LOCAL I-LOCAL I-LOCAL B-ORGANIZACAO I-ORGANIZACAO I-ORGANIZACAO O  O        O B-LOCAL I-LOCAL  I-LOCAL I-LOCAL O O   O B-LOCAL I-LOCAL O O   O          O O    O  O              O B-LOCAL I-LOCAL  O O   O O      O B-JURISPRUDENCIA I-JURISPRUDENCIA I-JURISPRUDENCIA          O         O          O  O       O         O  O     O  O     O       O \n"
     ]
    }
   ],
   "source": [
    "def show_graph_labels(dataset, index):\n",
    "    labels_model = [\n",
    "        \"O\",\n",
    "        \"B-ORGANIZACAO\",\n",
    "        \"I-ORGANIZACAO\",\n",
    "        \"B-PESSOA\",\n",
    "        \"I-PESSOA\",\n",
    "        \"B-TEMPO\",\n",
    "        \"I-TEMPO\",\n",
    "        \"B-LOCAL\",\n",
    "        \"I-LOCAL\",\n",
    "        \"B-LEGISLACAO\",\n",
    "        \"I-LEGISLACAO\",\n",
    "        \"B-JURISPRUDENCIA\",\n",
    "        \"I-JURISPRUDENCIA\",\n",
    "    ]\n",
    "    words = dataset[\"train\"][index][\"tokens\"]\n",
    "    labels = dataset[\"train\"][index][\"ner_tags\"]\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for word, label in zip(words, labels):\n",
    "        full_label = labels_model[label]\n",
    "        max_length = max(len(word), len(full_label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "    print(line1)\n",
    "    print(line2)\n",
    "\n",
    "\n",
    "show_graph_labels(dataset, 422)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar os dados sintéticos que serão inseridos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados com Faker\n",
    "\n",
    "- https://faker.readthedocs.io/en/master/#providers\n",
    "- https://faker.readthedocs.io/en/master/locales/pt_BR.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Faker==25.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'142.780.639-43'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def alterar_digito_verificador_cpf(cpf: str) -> str:\n",
    "    cpf = list(cpf)\n",
    "    cpf[12] = str((int(cpf[12]) + 1) % 10)\n",
    "    return \"\".join(cpf)\n",
    "\n",
    "\n",
    "# alterar_digito_verificador_cpf(\"142.780.639-33\")\n",
    "alterar_digito_verificador_cpf(\"142.780.639-33\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1427863933'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def aleatoriamente_remove_formatacao_cpf(cpf: str) -> str:\n",
    "    is_to_remove = random.choice([True, False])\n",
    "    if not is_to_remove:\n",
    "        return cpf\n",
    "    cpf = cpf.replace(\".\", \"\").replace(\"-\", \"\")\n",
    "    cpf = list(cpf)\n",
    "    cpf.remove(cpf[5])\n",
    "    return \"\".join(cpf)\n",
    "\n",
    "\n",
    "aleatoriamente_remove_formatacao_cpf(\"142.780.639-33\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Novaes'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "\n",
    "def aleatoriamente_seleciona_um_provider_da_faker_lib(\n",
    "    providers=[\"state\", \"street_address\", \"city\", \"bairro\"],\n",
    "):\n",
    "    provider = random.choice(providers)\n",
    "    return Faker(\"pt_BR\").format(provider)\n",
    "\n",
    "\n",
    "aleatoriamente_seleciona_um_provider_da_faker_lib()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sr. Athur Gabriel Carvalho\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def digitacao_errada(\n",
    "    texto: str, opcoes=[\"espaco\", \"troca\", \"insercao\", \"remocao\"]\n",
    ") -> str:\n",
    "    texto = list(texto)\n",
    "    tipo_erro = random.choice(opcoes)\n",
    "\n",
    "    if tipo_erro == \"espaco\":\n",
    "        posicao = random.randint(0, len(texto) - 1)\n",
    "        texto.insert(posicao, \" \")\n",
    "\n",
    "    elif tipo_erro == \"troca\" and len(texto) > 1:\n",
    "        posicao = random.randint(0, len(texto) - 2)\n",
    "        texto[posicao], texto[posicao + 1] = texto[posicao + 1], texto[posicao]\n",
    "\n",
    "    elif tipo_erro == \"insercao\":\n",
    "        posicao = random.randint(0, len(texto) - 1)\n",
    "        char_inserido = random.choice(\n",
    "            \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "        )\n",
    "        texto.insert(posicao, char_inserido)\n",
    "\n",
    "    elif tipo_erro == \"remocao\" and len(texto) > 1:\n",
    "        posicao = random.randint(0, len(texto) - 1)\n",
    "        texto.pop(posicao)\n",
    "\n",
    "    return \"\".join(texto)\n",
    "\n",
    "\n",
    "# Testando a função\n",
    "print(digitacao_errada(\"Sr. Arthur Gabriel Carvalho\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NOME': ['aMriah Moreira', 'DaviT Miguel da Paz'],\n",
       " 'DATA': ['03/07/19H71', 'jul ho de 1979'],\n",
       " 'CPF': ['3168450736', '9425310816'],\n",
       " 'TELEFONE': ['+55 (071 ) 8490-9369', '+55 (051 5018 9102'],\n",
       " 'EMAIL': ['mari-asophia78@example.org', 'vargasehnrique@uol.com.br'],\n",
       " 'DINHEIRO': ['R$ 54s,42', '8972'],\n",
       " 'CEP': [' 72529-428', '2867176 7'],\n",
       " 'ENDERECO': ['Cosat', 'Vasconcel os da Praia']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "\n",
    "def generate_faker_sets(\n",
    "    qnt=10,\n",
    "    qnt_variavel=False,\n",
    "    qnt_min=1,\n",
    "    exibir_todos=True,\n",
    "    is_fixed=False,\n",
    "    has_digitacao_errada=False,\n",
    "):\n",
    "    fake = Faker(\"pt_BR\")\n",
    "\n",
    "    if is_fixed:\n",
    "        random.seed(0)\n",
    "        Faker.seed(0)\n",
    "\n",
    "    date_pattern_br = \"%d/%m/%Y\"\n",
    "\n",
    "    if qnt_variavel:\n",
    "        qnt += 1\n",
    "\n",
    "    qnt_final = random.randrange(qnt_min, qnt) if qnt_variavel else qnt\n",
    "    qnt_metade_1 = round(qnt_final / 2)\n",
    "    qnt_metade_2 = qnt_final - qnt_metade_1\n",
    "\n",
    "    endereco_random_provider = random.choice(\n",
    "        [\"state\", \"street_address\", \"city\", \"bairro\"]\n",
    "    )\n",
    "\n",
    "    new_obj = {\n",
    "        \"NOME\": [fake.name() for _ in range(qnt_final)],\n",
    "        \"DATA\": [fake.date(pattern=date_pattern_br) for _ in range(qnt_metade_1)]\n",
    "        + [f\"{fake.month_name()} de {fake.year()}\" for _ in range(qnt_metade_2)],\n",
    "        \"CPF\": [\n",
    "            aleatoriamente_remove_formatacao_cpf(\n",
    "                alterar_digito_verificador_cpf(fake.cpf())\n",
    "            )\n",
    "            for _ in range(qnt_final)\n",
    "        ],\n",
    "        \"TELEFONE\": [fake.phone_number() for _ in range(qnt_final)],\n",
    "        \"EMAIL\": [fake.email() for _ in range(qnt_metade_1)]\n",
    "        + [fake.free_email() for _ in range(qnt_metade_2)],\n",
    "        \"DINHEIRO\": [fake.pricetag().replace(\"R$\", \"R$ \") for _ in range(qnt_metade_1)]\n",
    "        + [f'{fake.pricetag().replace(\"R$\", \"\")[:-3]}' for _ in range(qnt_metade_2)],\n",
    "        \"CEP\": [fake.postcode() for _ in range(qnt_final)],\n",
    "        \"ENDERECO\": [fake.format(endereco_random_provider) for _ in range(qnt_final)],\n",
    "    }\n",
    "\n",
    "    if not exibir_todos:\n",
    "        # Aleatoriamente excluia uma ou mais chaves\n",
    "        manter_qnt_chaves = 2\n",
    "        for _ in range(random.randint(1, len(new_obj) - manter_qnt_chaves)):\n",
    "            key = random.choice(list(new_obj.keys()))\n",
    "            del new_obj[key]\n",
    "\n",
    "    if has_digitacao_errada:\n",
    "        for key in new_obj:\n",
    "            new_obj[key] = [digitacao_errada(value) for value in new_obj[key]]\n",
    "\n",
    "    return new_obj\n",
    "\n",
    "\n",
    "# =========\n",
    "\n",
    "FAKE_DATA = generate_faker_sets(2, has_digitacao_errada=True)\n",
    "\n",
    "# FAKE_DATA = generate_faker_sets(5, qnt_variavel=True, exibir_todos=True)\n",
    "# FAKE_DATA = generate_faker_sets(3, qnt_variavel=True, exibir_todos=False)\n",
    "# FAKE_DATA = generate_faker_sets(3, is_fixed=True)\n",
    "FAKE_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- NOME: Luara Cassiano\\n- DATA: agosto de 1994\\n- CPF: 145.283.067-45\\n- TELEFONE: 11 8962-8517\\n- EMAIL: jpereira@gmail.com\\n- DINHEIRO: 62\\n- CEP: 14212-374\\n- ENDERECO: Lima de Melo\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fake_data_formatted(fake_data={}):\n",
    "    all_str = \"\"\n",
    "    for k, v in fake_data.items():\n",
    "        # if not v:\n",
    "        #     continue\n",
    "        v_rep = \"; \".join(v).replace(\"\\n\", \"; \")\n",
    "        all_str += f\"- {k}: {v_rep}\\n\"\n",
    "    return all_str\n",
    "\n",
    "\n",
    "# fake_data_formatted(FAKE_DATA)\n",
    "fake_data_formatted(generate_faker_sets(2, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando texto Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/exemplos_por_assunto.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m exemplos_por_assunto \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./dataset/exemplos_por_assunto.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m exemplos_por_assunto\n",
      "File \u001b[0;32m/home/celiudos/miniconda3/envs/trc/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/exemplos_por_assunto.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "exemplos_por_assunto = json.loads(\n",
    "    open(\"./dataset/exemplos_por_assunto.json\", \"r\").read()\n",
    ")\n",
    "exemplos_por_assunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  =============\n",
    "\n",
    "PROMPT_LLAMA = \"\"\"\n",
    "Você é um cidadão brasileiro.\n",
    "Você criará uma denúncia (campo DENUNCIA) ao Ministério Público Federal.\n",
    "O assunto da denúncia deve conter o seguinte tema: {tema_assunto}.\n",
    "Seja objetivo e direto.\n",
    "A denúncia deve ter entre 500 a 1000 caracteres.\n",
    "A denúncia conterá todos valores dispostos abaixo em DADOS_SINTETICOS.\n",
    "Não crie novos valores, utilize apenas os valores constantes em DADOS_SINTETICOS.\n",
    "Utilize os valores disponíveis em DADOS_SINTETICOS mantendo exatamente o mesmo formato e valor do texto.\n",
    "\n",
    "DADOS_SINTETICOS: {dados_sinteticos}\n",
    "\n",
    "DENUNCIA: \n",
    "\"\"\"\n",
    "# ===============\n",
    "\n",
    "tema_assunto = exemplos_por_assunto[\"penal\"][\"temas\"][0]\n",
    "dados_sinteticos = generate_faker_sets(\n",
    "    3, qnt_variavel=True, exibir_todos=False, has_digitacao_errada=True\n",
    ")\n",
    "\n",
    "exemplo_de_prompt = PROMPT_LLAMA.format(\n",
    "    dados_sinteticos=dados_sinteticos, tema_assunto=tema_assunto\n",
    ")\n",
    "\n",
    "print(exemplo_de_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o ChatGPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [URL para ver consumo do ChatGPT](https://platform.openai.com/usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# import json\n",
    "\n",
    "# client = OpenAI()\n",
    "# # MODEL_GPT = \"gpt-4-turbo\"\n",
    "# # MODEL_GPT = \"gpt-3.5-turbo-0125\"\n",
    "# # MODEL_GPT = \"gpt-3.5-turbo\"\n",
    "# MODEL_GPT = \"gpt-4o\"\n",
    "\n",
    "\n",
    "# def output_llm(question):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=MODEL_GPT,\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"system\",\n",
    "#                 \"content\": \"Você é um assistente útil projetado para gerar dados sintéticos.\",\n",
    "#             },\n",
    "#             {\"role\": \"user\", \"content\": question},\n",
    "#         ],\n",
    "#     )\n",
    "\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# # ============\n",
    "\n",
    "# resposta_chatgpt = output_llm(exemplo_de_prompt)\n",
    "# print(resposta_chatgpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o Llama v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import requests\n",
    "import json\n",
    "\n",
    "URL_API = \"http://127.0.0.1:8001\"\n",
    "MODEL_RESP_INFO = requests.get(f\"{URL_API}/info\").text\n",
    "\n",
    "client = InferenceClient(model=URL_API)\n",
    "\n",
    "\n",
    "def output_llm(message):\n",
    "    response = client.text_generation(\n",
    "        message,\n",
    "        max_new_tokens=1024 * 4,\n",
    "        stream=False,\n",
    "        repetition_penalty=1,\n",
    "        temperature=0.2,\n",
    "        stop_sequences=[\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|eot_id|>\",\n",
    "            \"\\n\\n\",\n",
    "        ],\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "# ============\n",
    "resposta_llama3 = output_llm(exemplo_de_prompt)\n",
    "resposta_llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_all_labels_are_in_the_text(text, labels={}, is_print_erros=True):\n",
    "    text = text.lower()\n",
    "    is_all_ok = True\n",
    "    for k, v in labels.items():\n",
    "        for label in v:\n",
    "            if label.lower() not in text:\n",
    "                if is_print_erros:\n",
    "                    print(f\"\\nLabel '{label}' from {k} not found in the text\")\n",
    "                is_all_ok = False\n",
    "\n",
    "    # print(\"All labels are in the text\")\n",
    "    return is_all_ok\n",
    "\n",
    "\n",
    "# ==============\n",
    "# check_if_all_labels_are_in_the_text(resposta_chatgpt, dados_sinteticos)\n",
    "check_if_all_labels_are_in_the_text(resposta_llama3, dados_sinteticos)\n",
    "# check_if_all_labels_are_in_the_text(\n",
    "#     \"EMENTA : APELAÇÃO CÍVEL - AÇÃO DE INDENIZAÇÃO POR DANOS MORAIS - PRELIMINAR - ARGUIDA PELO MINISTÉRIO PÚBLICO EM GRAU RECURSAL - NULIDADE - AUSÊNCIA DE INTERVENÇÃO DO PARQUET NA INSTÂNCIA A QUO - PRESENÇA DE INCAPAZ - PREJUÍZO EXISTENTE - PRELIMINAR ACOLHIDA - NULIDADE RECONHECIDA. O autor, Daniel Mendes, nascido em dezembro de 1990, com CPF 490.183.567-10 e telefone 0800 170 6459, é proprietário do endereço Praia Antônio Caldeira, 4, no CEP 28866-051. Ele também é dono da conta bancária com saldo de R$ 43,95 e pode ser contatado pelo e-mail santosbarbara@example.net. O Ministério Público, representado pelo com CPF 127.034.685-81 e telefone 61 6556 4995, e-mail frezende@example.net, também pode ser contatado. O valor da indenização é de R$ 3,58 e o autor tem um prazo de 30 dias para depositar o dinheiro no banco.\",\n",
    "#     dados_sinteticos,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_labels_are_not_in_the_text(text, labels={}):\n",
    "    new_labels = {}\n",
    "    for k, v in labels.items():\n",
    "        new_labels[k] = [l for l in v if l in text]\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "# ==============\n",
    "# new_labels = remove_all_labels_are_not_in_the_text(resposta_chatgpt, dados_sinteticos)\n",
    "new_labels = remove_all_labels_are_not_in_the_text(resposta_llama3, dados_sinteticos)\n",
    "# new_labels = remove_all_labels_are_not_in_the_text(\n",
    "#     '\"EMENTA : APELAÇÃO CÍVEL - AÇÃO DE INDENIZAÇÃO POR DANOS MORAIS - PRELIMINAR - ARGUIDA PELO MINISTÉRIO PÚBLICO EM GRAU RECURSAL - NULIDADE - AUSÊNCIA DE INTERVENÇÃO DO PARQUET NA INSTÂNCIA A QUO - PRESENÇA DE INCAPAZ - PREJUÍZO EXISTENTE - PRELIMINAR ACOLHIDA - NULIDADE RECONHECIDA. O autor da ação, Gabriel Lima, nascido em março de 1984, com CPF 059.184.723-14, telefone 61 6883-8430 e e-mail benicio22@example.org, reside na Rua Residencial de Moraes, 24, no CEP 25842-407. Ele alega ter sofrido danos morais no valor de R$ 18.386,99 e pede indenização. O Ministério Público, por sua vez, arguiu a nulidade da ação por falta de intervenção do parquet na instância a quo. Acolhida a preliminar, a nulidade foi reconhecida.\".\\n\\n',\n",
    "#     PROMPT_LLAMA_DADOS_SINTETICOS,\n",
    "# )\n",
    "print(\"Antes: \", dados_sinteticos)\n",
    "print(\"Depois: \", new_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerar dados Fakes Anotados com IOB tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Qnt dataset: \", len(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 texto = 5s\n",
    "# 250 textos = ~17min 30s\n",
    "\n",
    "DADOS_DATASET_DE = 0\n",
    "DADOS_DATASET_QNT = 100\n",
    "\n",
    "# =======\n",
    "\n",
    "PROMPT_VERSION = 2\n",
    "DESCRIPTION = \"qnt_variavel\"\n",
    "LLM = \"llamaV3\"\n",
    "\n",
    "# =======\n",
    "\n",
    "DADOS_DATASET_ATE = DADOS_DATASET_DE + DADOS_DATASET_QNT\n",
    "\n",
    "OUTPUT_PATH = f\"./output/dataset_sintetico_v12_{DADOS_DATASET_DE}_{DADOS_DATASET_ATE}_com_erros_digitacao.csv\"\n",
    "print(\"OUTPUT_PATH: \", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_texto_dataset(dataset):\n",
    "#     texts = []\n",
    "#     for item in dataset[\"tokens\"]:\n",
    "#         texts.append(\" \".join(item))\n",
    "#     return texts\n",
    "\n",
    "\n",
    "# # ===============\n",
    "# DADOS_DATASET = get_texto_dataset(dataset[\"train\"][DADOS_DATASET_DE:DADOS_DATASET_ATE])\n",
    "# print(\"Qnt: \", len(DADOS_DATASET))\n",
    "# DADOS_DATASET[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(index, total):\n",
    "    progress = (index + 1) / total * 100\n",
    "    print(f\"\\rProgresso: {progress:.2f}%\", end=\"\")\n",
    "\n",
    "\n",
    "# =============\n",
    "show_progress(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inserir_dados_fake(\n",
    "    qnt_gerar=2,\n",
    "    output_llm_func=output_llm,\n",
    "    prompt_template=PROMPT_LLAMA,\n",
    "    generate_faker_sets_params={},\n",
    "):\n",
    "    dados_fake = []\n",
    "\n",
    "    for ind in range(qnt_gerar):\n",
    "        try:\n",
    "            dados_sinteticos = generate_faker_sets(**generate_faker_sets_params)\n",
    "\n",
    "            show_progress(ind, qnt_gerar)\n",
    "\n",
    "            tema_assunto = exemplos_por_assunto[\"penal\"][\"temas\"][0]\n",
    "            dados_sinteticos = generate_faker_sets(**generate_faker_sets_params)\n",
    "\n",
    "            prompt_gerado = prompt_template.format(\n",
    "                dados_sinteticos=dados_sinteticos, tema_assunto=tema_assunto\n",
    "            )\n",
    "\n",
    "            resposta_llm = output_llm_func(prompt_gerado)\n",
    "\n",
    "            is_all_labels_in = check_if_all_labels_are_in_the_text(\n",
    "                text=resposta_llm, labels=dados_sinteticos, is_print_erros=False\n",
    "            )\n",
    "\n",
    "            if not is_all_labels_in:\n",
    "                # print(f\"\\nLabels não encontrados no texto índice: {ind} ...ajustando\")\n",
    "                dados_sinteticos = remove_all_labels_are_not_in_the_text(\n",
    "                    resposta_llm, dados_sinteticos\n",
    "                )\n",
    "\n",
    "            dados_fake.append(\n",
    "                {\"texto\": resposta_llm, \"dados_sinteticos\": dados_sinteticos}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"\\nErro: \", e)\n",
    "            print(\"\\nprompt_gerado: \", prompt_gerado)\n",
    "            print(\"\\resposta_llm: \", resposta_llm)\n",
    "            # print(\"\\nDados Gerados: \", dados_fake)\n",
    "            continue\n",
    "\n",
    "    return dados_fake\n",
    "\n",
    "\n",
    "# =========\n",
    "\n",
    "generate_faker_sets_params = {\n",
    "    \"qnt\": 3,\n",
    "    \"qnt_variavel\": True,\n",
    "    \"exibir_todos\": False,\n",
    "    \"has_digitacao_errada\": False,\n",
    "}\n",
    "\n",
    "DADOS_FAKE = inserir_dados_fake(\n",
    "    qnt_gerar=DADOS_DATASET_ATE,\n",
    "    output_llm_func=output_llm,\n",
    "    generate_faker_sets_params=generate_faker_sets_params,\n",
    ")\n",
    "print(\"\\nQnt: \", len(DADOS_FAKE))\n",
    "DADOS_FAKE[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(DADOS_FAKE[1][\"texto\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(DADOS_FAKE)\n",
    "\n",
    "DESCRIPTION = str(generate_faker_sets_params)\n",
    "\n",
    "df[\"prompt_version\"] = PROMPT_VERSION\n",
    "df[\"description\"] = DESCRIPTION\n",
    "df[\"llm\"] = LLM\n",
    "\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"Qnt: \", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge com dados já existentes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_1 = pd.read_csv(\"./output/dataset_sintetico_v10.csv\")\n",
    "df_2 = pd.read_csv(\"./output/dataset_sintetico_v11.csv\")\n",
    "print(\"Qnt df_1: \", len(df_1))\n",
    "print(\"Qnt df_2: \", len(df_2))\n",
    "\n",
    "df_merged = pd.concat([df_1, df_2], ignore_index=True)\n",
    "df_merged.to_csv(\"./output/dataset_sintetico_v11.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 300)\n",
    "\n",
    "df_11 = pd.read_csv(\"./output/dataset_sintetico_v11.csv\")\n",
    "df_11.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_11[\"description\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserindo Dados já anotados do LenerBR no novo Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_tokens_inputs_ids_to_json(texto_ex, input_val, labels_model):\n",
    "#     tokens = input_val[0]\n",
    "#     labels = input_val[1]\n",
    "\n",
    "#     entities = {}\n",
    "#     current_entity_tokens = []\n",
    "#     current_entity_start = None\n",
    "#     current_label = None\n",
    "\n",
    "#     def get_entity_text(tokens, start_idx, texto_ex):\n",
    "#         end_idx = start_idx\n",
    "#         for token in tokens:\n",
    "#             end_idx = texto_ex.find(token, end_idx)\n",
    "#             end_idx += len(token)\n",
    "#         return texto_ex[start_idx:end_idx]\n",
    "\n",
    "#     for i, (token, label_id) in enumerate(zip(tokens, labels)):\n",
    "#         label = labels_model[label_id]\n",
    "\n",
    "#         if label.startswith(\"B-\"):\n",
    "#             if current_label:\n",
    "#                 entity_text = get_entity_text(\n",
    "#                     current_entity_tokens, current_entity_start, texto_ex\n",
    "#                 )\n",
    "#                 entities[current_label].append(entity_text)\n",
    "#             current_label = label[2:]\n",
    "#             current_entity_tokens = [token]\n",
    "#             current_entity_start = texto_ex.find(\n",
    "#                 token, current_entity_start if current_entity_start else 0\n",
    "#             )\n",
    "#             if current_label not in entities:\n",
    "#                 entities[current_label] = []\n",
    "#         elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "#             current_entity_tokens.append(token)\n",
    "#         else:\n",
    "#             if current_label:\n",
    "#                 entity_text = get_entity_text(\n",
    "#                     current_entity_tokens, current_entity_start, texto_ex\n",
    "#                 )\n",
    "#                 entities[current_label].append(entity_text)\n",
    "#             current_entity_tokens = []\n",
    "#             current_label = None\n",
    "\n",
    "#     if current_label:\n",
    "#         entity_text = get_entity_text(\n",
    "#             current_entity_tokens, current_entity_start, texto_ex\n",
    "#         )\n",
    "#         entities[current_label].append(entity_text)\n",
    "\n",
    "#     return entities\n",
    "\n",
    "\n",
    "# # =============\n",
    "# labels_model = [\n",
    "#     \"O\",\n",
    "#     \"B-NOME\",\n",
    "#     \"I-NOME\",\n",
    "#     \"B-DATA\",\n",
    "#     \"I-DATA\",\n",
    "#     \"B-ENDERECO\",\n",
    "#     \"I-ENDERECO\",\n",
    "#     \"B-CPF\",\n",
    "#     \"I-CPF\",\n",
    "#     \"B-TELEFONE\",\n",
    "#     \"I-TELEFONE\",\n",
    "#     \"B-EMAIL\",\n",
    "#     \"I-EMAIL\",\n",
    "#     \"B-DINHEIRO\",\n",
    "#     \"I-DINHEIRO\",\n",
    "#     \"B-CEP\",\n",
    "#     \"I-CEP\",\n",
    "# ]\n",
    "\n",
    "# texto_ex = \"Marinalva Bete Raz e Jorge Luiz receberam R$ 3.829,83 reais.\"\n",
    "# input_val = [\n",
    "#     [\n",
    "#         \"Marinalva\",\n",
    "#         \"Bete\",\n",
    "#         \"Raz\",\n",
    "#         \"e\",\n",
    "#         \"Jorge\",\n",
    "#         \"Luiz\",\n",
    "#         \"receberam\",\n",
    "#         \"R\",\n",
    "#         \"$\",\n",
    "#         \"3\",\n",
    "#         \".\",\n",
    "#         \"829\",\n",
    "#         \",\",\n",
    "#         \"83\",\n",
    "#         \"reais\",\n",
    "#         \".\",\n",
    "#     ],\n",
    "#     [1, 2, 2, 0, 1, 2, 0, 13, 14, 14, 14, 14, 14, 14, 0, 0],\n",
    "# ]\n",
    "# output_esperado = {\n",
    "#     \"NOME\": [\"Marinalva Bete Raz\", \"Jorge Luiz\"],\n",
    "#     \"DINHEIRO\": [\"R$ 3.829,83\"],\n",
    "# }\n",
    "\n",
    "# result = convert_tokens_inputs_ids_to_json(texto_ex, input_val, labels_model)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"lener_br\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_labels_from_dataset(dataset, index):\n",
    "#     labels_model = [\n",
    "#         \"O\",\n",
    "#         \"B-ORGANIZACAO\",\n",
    "#         \"I-ORGANIZACAO\",\n",
    "#         \"B-PESSOA\",\n",
    "#         \"I-PESSOA\",\n",
    "#         \"B-TEMPO\",\n",
    "#         \"I-TEMPO\",\n",
    "#         \"B-LOCAL\",\n",
    "#         \"I-LOCAL\",\n",
    "#         \"B-LEGISLACAO\",\n",
    "#         \"I-LEGISLACAO\",\n",
    "#         \"B-JURISPRUDENCIA\",\n",
    "#         \"I-JURISPRUDENCIA\",\n",
    "#     ]\n",
    "\n",
    "#     texto_ex = \" \".join(dataset[\"tokens\"][index])\n",
    "#     input_val = [\n",
    "#         dataset[\"tokens\"][index],\n",
    "#         dataset[\"ner_tags\"][index],\n",
    "#     ]\n",
    "#     return convert_tokens_inputs_ids_to_json(texto_ex, input_val, labels_model)\n",
    "\n",
    "\n",
    "# # ===========\n",
    "\n",
    "# get_labels_from_dataset(dataset[\"train\"], 422)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dos Dados NER do Lener no nosso Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(\"./output/dataset_sintetico_v7_0_700.csv\")\n",
    "# print(\"Qnt: \", len(df))\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[\"texto\"][422])\n",
    "# print(df[\"dados_sinteticos\"][422])\n",
    "# print(get_labels_from_dataset(dataset[\"train\"], 422))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "\n",
    "# def merge_entities(dataset_origin, dataset_generated, index):\n",
    "#     ori_dict = get_labels_from_dataset(dataset_origin, index)\n",
    "#     gen_dict = json.loads(dataset_generated[index].replace(\"'\", '\"'))\n",
    "#     gen_dict_original = json.loads(dataset_generated[index].replace(\"'\", '\"'))\n",
    "\n",
    "#     merges_from_to = {\n",
    "#         \"PESSOA\": \"NOME\",\n",
    "#         \"TEMPO\": \"DATA\",\n",
    "#         \"LOCAL\": \"ENDERECO\",\n",
    "#     }\n",
    "\n",
    "#     # insert into gen_dict the ori_dict only from \"merges_from_to\" and add data\n",
    "#     for k, v in merges_from_to.items():\n",
    "#         if k in ori_dict:\n",
    "#             gen_dict[v] += ori_dict[k]\n",
    "#             # unique the values\n",
    "#             gen_dict[v] = list(set(gen_dict[v]))\n",
    "\n",
    "#     return ori_dict, gen_dict_original, gen_dict\n",
    "\n",
    "\n",
    "# # ==========\n",
    "\n",
    "# merge_entities(dataset[\"train\"], df[\"dados_sinteticos\"], 420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rodando todo o processo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rode o \"merge_entities\" para todos os itens do \"df\" and add a new col to \"df\" with them called \"dados_sinteticos_merged\"\n",
    "# def merge_all_entities(dataset_origin, dataset_generated):\n",
    "#     all_entities = []\n",
    "#     for i in range(len(dataset_generated)):\n",
    "#         _, _, gen_dict = merge_entities(dataset_origin, dataset_generated, i)\n",
    "#         all_entities.append(gen_dict)\n",
    "#     return all_entities\n",
    "\n",
    "\n",
    "# # =============\n",
    "\n",
    "# df[\"dados_sinteticos_merged\"] = merge_all_entities(\n",
    "#     dataset[\"train\"], df[\"dados_sinteticos\"]\n",
    "# )\n",
    "# df.to_csv(\"./output/dataset_sintetico_v7_0_700_merged.csv\", index=False)\n",
    "# df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df_all = pd.read_csv(\"./output/dataset_sintetico_v7_0_700_merged.csv\")\n",
    "# df_all[\"dados_sinteticos\"][422]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all[\"dados_sinteticos_merged\"][422]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo textos grandes (provavelmente com erros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # df = pd.DataFrame(DADOS_FAKE)\n",
    "# df_all = pd.read_csv(\"./output/dataset_sintetico_v7_0_700.csv\")\n",
    "# print(\"Qnt: \", len(df_all))\n",
    "# df_all.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show describe from text size\n",
    "# df_all[\"texto\"].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show more text from pandas\n",
    "# pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "# # removing rows with col \"text\" more then 1024\n",
    "# df_all_n = df_all[df_all[\"texto\"].str.len() < 1024]\n",
    "# print(\"Qnt: \", len(df_all_n))\n",
    "# # df_all_n[\"texto\"].head(5)\n",
    "# # df_all_n[\"texto\"].to_list()[:10]\n",
    "# df_all_n.to_csv(\"./output/dataset_sintetico_v7_0_700.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
