{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criar um Dataset Anotado Sintético com dados para avaliar LGPD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fontes\n",
    "\n",
    "**GOLD**\n",
    "\n",
    "- 2024 Artigo - Combining prompt‑based language models and weak supervision for labeling named entity recognition on legal documents\n",
    "- 2024 Artigo - DODFMiner: An automated tool for Named Entity Recognition from Official Gazettes\n",
    "- 2024 Artigo - Artigo - Legal Document Segmentation and Labeling Through Named Entity Recognition\n",
    "\n",
    "**BRONZE**\n",
    "\n",
    "- 2022 Artigo - Do We Still Need Human Assessors? Prompt-Based GPT-3 User Simulation in Conversational AI (...We come to the conclusion that, although classifiers trained on such synthetic data perform much better than random baselines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Medium - Pierre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonte: [Post Medium Pierre](https://medium.com/@pierre_guillou/nlp-modelos-e-web-app-para-reconhecimento-de-entidade-nomeada-ner-no-dom%C3%ADnio-jur%C3%ADdico-b658db55edfb)\n",
    "\n",
    "- [Modelo HF](https://huggingface.co/pierreguillou/ner-bert-large-cased-pt-lenerbr)\n",
    "\n",
    "Modelos de linguagem natural especializados no domínio jurídico brasileiro Para obter o modelo de linguagem natural especializado no domínio jurídico brasileiro, usamos o notebook e dataset a seguir:\n",
    "\n",
    "- notebook: [Finetuning_language_model_BERtimbau_LeNER_Br.ipynb](https://github.com/piegu/language-models/blob/master/Finetuning_language_model_BERtimbau_LeNER_Br.ipynb)\n",
    "- dataset: [pierreguillou/lener_br_finetuning_language_model](https://huggingface.co/datasets/pierreguillou/lener_br_finetuning_language_model)\n",
    "\n",
    "Modelos NER especializados no domínio jurídico brasileiro Para obter o modelo NER especializado no domínio jurídico brasileiro, usamos o notebook e dataset a seguir:\n",
    "\n",
    "- notebook: [HuggingFace_Notebook_token_classification_NER_LeNER_Br.ipynb](https://github.com/piegu/language-models/blob/master/HuggingFace_Notebook_token_classification_NER_LeNER_Br.ipynb)\n",
    "- dataset: [lener_br](https://huggingface.co/datasets/lener_br)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HF IOB](https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixando a SEED para reprodução\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def fixando_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ===\n",
    "SEED = 42\n",
    "fixando_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar o Dataset LenerBR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"lener_br\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" \".join(dataset[\"train\"][\"tokens\"][422])\n",
    "# dataset[\"train\"][\"tokens\"][422]\n",
    "print(dataset[\"train\"][422])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph_labels(dataset, index):\n",
    "    labels_model = [\n",
    "        \"O\",\n",
    "        \"B-ORGANIZACAO\",\n",
    "        \"I-ORGANIZACAO\",\n",
    "        \"B-PESSOA\",\n",
    "        \"I-PESSOA\",\n",
    "        \"B-TEMPO\",\n",
    "        \"I-TEMPO\",\n",
    "        \"B-LOCAL\",\n",
    "        \"I-LOCAL\",\n",
    "        \"B-LEGISLACAO\",\n",
    "        \"I-LEGISLACAO\",\n",
    "        \"B-JURISPRUDENCIA\",\n",
    "        \"I-JURISPRUDENCIA\",\n",
    "    ]\n",
    "    words = dataset[\"train\"][index][\"tokens\"]\n",
    "    labels = dataset[\"train\"][index][\"ner_tags\"]\n",
    "    line1 = \"\"\n",
    "    line2 = \"\"\n",
    "    for word, label in zip(words, labels):\n",
    "        full_label = labels_model[label]\n",
    "        max_length = max(len(word), len(full_label))\n",
    "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "        line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "    print(line1)\n",
    "    print(line2)\n",
    "\n",
    "\n",
    "show_graph_labels(dataset, 422)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar os dados sintéticos que serão inseridos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados com Faker\n",
    "\n",
    "- https://faker.readthedocs.io/en/master/#providers\n",
    "- https://faker.readthedocs.io/en/master/locales/pt_BR.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Faker==25.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alterar_digito_verificador_cpf(cpf: str) -> str:\n",
    "    cpf = list(cpf)\n",
    "    cpf[12] = str((int(cpf[12]) + 1) % 10)\n",
    "    return \"\".join(cpf)\n",
    "\n",
    "\n",
    "alterar_digito_verificador_cpf(\"142.780.639-33\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "\n",
    "def generate_faker_sets(qnt=10, qnt_variavel=False, qnt_min=1):\n",
    "    fake = Faker(\"pt_BR\")\n",
    "\n",
    "    date_pattern_br = \"%d/%m/%Y\"\n",
    "\n",
    "    if qnt_variavel:\n",
    "        qnt += 1\n",
    "\n",
    "    qnt_final = random.randrange(qnt_min, qnt) if qnt_variavel else qnt\n",
    "    qnt_metade_1 = round(qnt_final / 2)\n",
    "    qnt_metade_2 = qnt_final - qnt_metade_1\n",
    "\n",
    "    new_obj = {\n",
    "        \"NOME\": [\n",
    "            fake.name()\n",
    "            for _ in range(random.randrange(qnt_min, qnt) if qnt_variavel else qnt)\n",
    "        ],\n",
    "        \"DATA\": [fake.date(pattern=date_pattern_br) for _ in range(qnt_metade_1)]\n",
    "        + [f\"{fake.month_name()} de {fake.year()}\" for _ in range(qnt_metade_2)],\n",
    "        \"CPF\": [\n",
    "            alterar_digito_verificador_cpf(fake.cpf())\n",
    "            for _ in range(random.randrange(qnt_min, qnt) if qnt_variavel else qnt)\n",
    "        ],\n",
    "        \"TELEFONE\": [\n",
    "            fake.phone_number()\n",
    "            for _ in range(random.randrange(qnt_min, qnt) if qnt_variavel else qnt)\n",
    "        ],\n",
    "        \"EMAIL\": [\n",
    "            fake.email()\n",
    "            for _ in range(random.randrange(qnt_min, qnt) if qnt_variavel else qnt)\n",
    "        ],\n",
    "        \"DINHEIRO\": [\n",
    "            fake.pricetag().replace(\"R$\", \"R$ \")\n",
    "            for _ in range(random.randrange(qnt_min, qnt) if qnt_variavel else qnt)\n",
    "        ],\n",
    "        \"CEP\": [\n",
    "            fake.postcode()\n",
    "            for _ in range(random.randrange(qnt_min, qnt) if qnt_variavel else qnt)\n",
    "        ],\n",
    "        \"ENDERECO\": [fake.state() for _ in range(qnt_metade_1)]\n",
    "        + [fake.street_address() for _ in range(qnt_metade_2)],\n",
    "    }\n",
    "\n",
    "    return new_obj\n",
    "\n",
    "\n",
    "# =========\n",
    "\n",
    "# FAKE_DATA = generate_faker_sets(15)\n",
    "FAKE_DATA = generate_faker_sets(2, True)\n",
    "FAKE_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_data_formatted(fake_data={}):\n",
    "    all_str = \"\"\n",
    "    for k, v in fake_data.items():\n",
    "        # if not v:\n",
    "        #     continue\n",
    "        v_rep = \"; \".join(v).replace(\"\\n\", \"; \")\n",
    "        all_str += f\"- {k}: {v_rep}\\n\"\n",
    "    return all_str\n",
    "\n",
    "\n",
    "# fake_data_formatted(FAKE_DATA)\n",
    "fake_data_formatted(generate_faker_sets(2, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando texto Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT_LLAMA_DADOS_SINTETICOS = fake_data_formatted(generate_faker_sets(2, True))\n",
    "# PROMPT_LLAMA_DADOS_SINTETICOS = fake_data_formatted(generate_faker_sets(2))\n",
    "# PROMPT_LLAMA_DADOS_SINTETICOS = generate_faker_sets(2)\n",
    "# PROMPT_LLAMA_DADOS_SINTETICOS = generate_faker_sets(2, True)\n",
    "\n",
    "# PROMPT_LLAMA_TEXTO = \" \".join(dataset[\"train\"][\"tokens\"][10])\n",
    "\n",
    "# ============= MANUAL\n",
    "\n",
    "PROMPT_LLAMA_TEXTO = f\"\"\"EMENTA : APELAÇÃO CÍVEL - AÇÃO DE INDENIZAÇÃO POR DANOS MORAIS - PRELIMINAR - ARGUIDA PELO MINISTÉRIO PÚBLICO EM GRAU RECURSAL - NULIDADE - AUSÊNCIA DE INTERVENÇÃO DO PARQUET NA INSTÂNCIA A QUO - PRESENÇA DE INCAPAZ - PREJUÍZO EXISTENTE - PRELIMINAR ACOLHIDA - NULIDADE RECONHECIDA\"\"\"\n",
    "PROMPT_LLAMA_DADOS_SINTETICOS = {\n",
    "    \"NOME\": [\"Daniel Mendes\"],\n",
    "    \"DATA\": [\"dezembro de 1990\"],\n",
    "    \"CPF\": [\"490.183.567-10\", \"127.034.685-81\"],\n",
    "    \"TELEFONE\": [\"0800 170 6459\", \"61 6556 4995\"],\n",
    "    \"EMAIL\": [\"santosbarbara@example.net\", \"frezende@example.net\"],\n",
    "    \"DINHEIRO\": [\"R$ 43,95\", \"R$ 3,58\"],\n",
    "    \"CEP\": [\"28866-051\", \"29566719\"],\n",
    "    \"ENDERECO\": [\"Praia Antônio Caldeira, 4\"],\n",
    "}\n",
    "\n",
    "# =============\n",
    "\n",
    "PROMPT_LLAMA = \"\"\"Você é um especialista em Processamento de Linguagem Natural.\n",
    "Você criará dados sintéticos.\n",
    "Dado o TEXTO abaixo, insira todos os DADOS_SINTETICOS no TEXTO para aumentá-lo.\n",
    "Não crie novos termos ou informações, apenas insira todos os DADOS_SINTETICOS no TEXTO.\n",
    "\n",
    "DADOS_SINTETICOS: {dados_sinteticos}\n",
    "\n",
    "TEXTO: \"{texto}\"\n",
    "\n",
    "Resposta com o TEXTO aumentado:\n",
    "\"\"\"\n",
    "# ===============\n",
    "\n",
    "exemplo_de_prompt = PROMPT_LLAMA.format(\n",
    "    texto=PROMPT_LLAMA_TEXTO, dados_sinteticos=PROMPT_LLAMA_DADOS_SINTETICOS\n",
    ")\n",
    "\n",
    "print(exemplo_de_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o Llama v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import requests\n",
    "\n",
    "URL_API = \"http://127.0.0.1:8001\"\n",
    "MODEL_RESP_INFO = requests.get(f\"{URL_API}/info\").text\n",
    "\n",
    "client = InferenceClient(model=URL_API)\n",
    "\n",
    "\n",
    "def output_llama3_json(message):\n",
    "    response = client.text_generation(\n",
    "        message,\n",
    "        max_new_tokens=512,\n",
    "        stream=False,\n",
    "        repetition_penalty=1,\n",
    "        temperature=0.2,\n",
    "        stop_sequences=[\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|eot_id|>\",\n",
    "            \"\\n\\n\",\n",
    "        ],\n",
    "        seed=SEED\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "# ============\n",
    "resposta_llama3 = output_llama3_json(exemplo_de_prompt)\n",
    "print(resposta_llama3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_all_labels_are_in_the_text(text, labels={}, is_print_erros=True):\n",
    "    text = text.lower()\n",
    "    is_all_ok = True\n",
    "    for k, v in labels.items():\n",
    "        for label in v:\n",
    "            if label.lower() not in text:\n",
    "                if is_print_erros:\n",
    "                    print(f\"\\nLabel '{label}' from {k} not found in the text\")\n",
    "                is_all_ok = False\n",
    "\n",
    "    # print(\"All labels are in the text\")\n",
    "    return is_all_ok\n",
    "\n",
    "\n",
    "# ==============\n",
    "check_if_all_labels_are_in_the_text(resposta_llama3, PROMPT_LLAMA_DADOS_SINTETICOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_labels_are_not_in_the_text(text, labels={}):\n",
    "    new_labels = {}\n",
    "    for k, v in labels.items():\n",
    "        new_labels[k] = [l for l in v if l in text]\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "# ==============\n",
    "new_labels = remove_all_labels_are_not_in_the_text(\n",
    "    resposta_llama3, PROMPT_LLAMA_DADOS_SINTETICOS\n",
    ")\n",
    "new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerar dados Fakes Anotados com IOB tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Qnt dataset: \", len(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 texto = 10s\n",
    "# 50 textos = ~4min 12s\n",
    "# 100 textos = ~12min\n",
    "# 500 textos = ~48min\n",
    "DADOS_DATASET_DE = 0\n",
    "DADOS_DATASET_QNT = 50\n",
    "\n",
    "# =======\n",
    "\n",
    "DADOS_DATASET_ATE = DADOS_DATASET_DE + DADOS_DATASET_QNT\n",
    "\n",
    "OUTPUT_PATH = (\n",
    "    f\"./output/dataset_sintetico_{DADOS_DATASET_DE}_{DADOS_DATASET_ATE}.csv\"\n",
    ")\n",
    "print(\"OUTPUT_PATH: \", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texto_dataset(dataset):\n",
    "    texts = []\n",
    "    for item in dataset[\"tokens\"]:\n",
    "        texts.append(\" \".join(item))\n",
    "    return texts\n",
    "\n",
    "\n",
    "# ===============\n",
    "DADOS_DATASET = get_texto_dataset(dataset[\"train\"][DADOS_DATASET_DE:DADOS_DATASET_ATE])\n",
    "print(\"Qnt: \", len(DADOS_DATASET))\n",
    "DADOS_DATASET[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(index, total):\n",
    "    progress = (index + 1) / total * 100\n",
    "    print(f\"\\rProgresso: {progress:.2f}%\", end=\"\")\n",
    "\n",
    "\n",
    "# =============\n",
    "show_progress(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inserir_dados_fake(dataset=[], qnt_fake_por_dados=2, qnt_fake_random=False):\n",
    "    dados_fake = []\n",
    "    dataset_len = len(dataset)\n",
    "\n",
    "    for ind, texto in enumerate(dataset):\n",
    "        try:\n",
    "            dados_sinteticos = generate_faker_sets(qnt_fake_por_dados, qnt_fake_random)\n",
    "\n",
    "            show_progress(ind, dataset_len)\n",
    "\n",
    "            prompt_gerado = PROMPT_LLAMA.format(\n",
    "                texto=texto, dados_sinteticos=dados_sinteticos\n",
    "            )\n",
    "            resposta_llama3 = output_llama3_json(prompt_gerado)\n",
    "\n",
    "            is_all_labels_in = check_if_all_labels_are_in_the_text(\n",
    "                text=resposta_llama3, labels=dados_sinteticos, is_print_erros=False\n",
    "            )\n",
    "\n",
    "            if not is_all_labels_in:\n",
    "                # print(f\"\\nLabels não encontrados no texto índice: {ind} ...ajustando\")\n",
    "                dados_sinteticos = remove_all_labels_are_not_in_the_text(\n",
    "                    resposta_llama3, dados_sinteticos\n",
    "                )\n",
    "\n",
    "            dados_fake.append(\n",
    "                {\"texto\": resposta_llama3, \"dados_sinteticos\": dados_sinteticos}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"\\nErro: \", e)\n",
    "            print(\"\\nprompt_gerado: \", prompt_gerado)\n",
    "            print(\"\\rresposta_llama3: \", resposta_llama3)\n",
    "            # print(\"\\nDados Gerados: \", dados_fake)\n",
    "            continue\n",
    "\n",
    "    return dados_fake\n",
    "\n",
    "\n",
    "# =========\n",
    "\n",
    "DADOS_FAKE = inserir_dados_fake(\n",
    "    DADOS_DATASET,\n",
    "    qnt_fake_por_dados=2,\n",
    "    qnt_fake_random=True,\n",
    "    # DADOS_DATASET,\n",
    "    # qnt_fake_por_dados=1,\n",
    "    # qnt_fake_random=False,\n",
    ")\n",
    "print(\"\\nQnt: \", len(DADOS_FAKE))\n",
    "DADOS_FAKE[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(DADOS_FAKE)\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"Qnt: \", len(df))\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
